classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = "final"
)
# --------------------- #
# ⚙️ 模型训练
# --------------------- #
set.seed(123)
glm_model <- train(label ~ ., data = model_data,
method = "glm", family = binomial(),
trControl = cv_control, metric = "ROC")
set.seed(123)
rf_model <- train(label ~ ., data = model_data,
method = "rf", tuneLength = 5,
trControl = cv_control, metric = "ROC")
set.seed(123)
gbm_model <- train(label ~ ., data = model_data,
method = "gbm", verbose = FALSE, tuneLength = 5,
trControl = cv_control, metric = "ROC")
set.seed(123)
xgb_model <- train(label ~ ., data = model_data,
method = "xgbTree", tuneLength = 5,
trControl = cv_control, metric = "ROC")
# ----------------------------- #
# 📊 提取性能指标（含 AUC_SD）
# ----------------------------- #
suppress_predict <- function(model, name){
suppressWarnings({
preds <- model$pred
tune_cols <- intersect(names(preds), names(model$bestTune))
# 只保留最佳超参数组合
if (length(tune_cols) > 0){
for (tc in tune_cols){
preds <- preds[preds[[tc]] == model$bestTune[[tc]], ]
}
}
# 每折 confusion matrix
cm_list <- lapply(split(preds, preds$Resample), function(df){
cm <- confusionMatrix(df$pred, df$obs, positive = "Progressor")
data.frame(
Accuracy = cm$overall["Accuracy"],
Sensitivity = cm$byClass["Sensitivity"],
Specificity = cm$byClass["Specificity"]
)
})
cm_df <- bind_rows(cm_list)
# 每折 AUC，强制转 numeric
auc_list <- preds %>%
group_by(Resample) %>%
summarise(
auc = as.numeric(roc(response = obs, predictor = Progressor,
levels = c("Stable", "Progressor"), direction = "<")$auc),
.groups = "drop"
)
auc_mean <- mean(auc_list$auc)
auc_sd   <- sd(auc_list$auc)
# 返回性能指标
data.frame(
Model = name,
AUC = auc_mean,
AUC_SD = auc_sd,
Accuracy = mean(cm_df$Accuracy),
Accuracy_SD = sd(cm_df$Accuracy),
Sensitivity = mean(cm_df$Sensitivity),
Sensitivity_SD = sd(cm_df$Sensitivity),
Specificity = mean(cm_df$Specificity),
Specificity_SD = sd(cm_df$Specificity)
)
})
}
perf_summary <- bind_rows(
suppress_predict(glm_model, "Logistic Regression"),
suppress_predict(rf_model,  "Random Forest"),
suppress_predict(gbm_model, "GBM"),
suppress_predict(xgb_model, "XGBoost")
)
perf_summary_rounded <- perf_summary %>%
mutate(across(where(is.numeric), ~ round(.x, 3)))
# 🧾 输出性能表格
perf_summary_rounded %>%
kable(caption = "5-Fold CV Model Performance Summary", align = 'c') %>%
kable_styling(full_width = FALSE)
# --------------------- #
# 📈 ROC 曲线可视化
# --------------------- #
get_roc_df <- function(model, name){
preds <- model$pred
tune_cols <- intersect(names(preds), names(model$bestTune))
if (length(tune_cols) > 0){
for (tc in tune_cols){
preds <- preds[preds[[tc]] == model$bestTune[[tc]], ]
}
}
roc_obj <- roc(response = preds$obs, predictor = preds$Progressor,
levels = c("Stable", "Progressor"), direction = "<")
data.frame(FPR = 1 - roc_obj$specificities,
TPR = roc_obj$sensitivities,
Model = name)
}
roc_df <- bind_rows(
get_roc_df(glm_model,  "Logistic Regression"),
get_roc_df(rf_model,   "Random Forest"),
get_roc_df(gbm_model,  "GBM"),
get_roc_df(xgb_model,  "XGBoost")
)
ggplot(roc_df, aes(x = FPR, y = TPR, color = Model)) +
geom_line(size = 1.2) +
geom_abline(linetype = "dashed", color = "gray") +
theme_minimal() +
labs(title = "MCI → AD Progression Prediction ROC Curves",
x = "False Positive Rate", y = "True Positive Rate") +
theme(legend.position = "bottom")
ADNI_table
library(dplyr)
library(caret)
library(pROC)
library(ggplot2)
library(knitr)
library(kableExtra)
library(devtools)
library(ADMerge)
# library(dplyr)
library(knitr)
# library(readxl)
library(kableExtra)
ADNI_table = get_src_table(path = "ADNI Demonstration",
ID_usr_list = c("RID"),
DATE_usr_list = c("EXAMDATE","VISDATE"),
non_longitudinal_list = c("APOERES_ADNI_1_2_3"),
IS_overlap_list = c("FALSE"),
WINDOW_list = c(366))
ADNI_table
plot_files(path = "ADNI Demonstration", dict_src = ADNI_table, study_type = "ADNI",date_type = "Date")
ADNI_merged = ad_merge(path = "ADNI_Demonstration",
DATE_type = "Date",
timeline_file = "ADNI_Cognitive",
dict_src = ADNI_table)
#
complete_ADNI = review_complete(ADNI_merged, check_cols = c(
# ---- 基本 ID / 时间 ----
"ID_merged",      # 合并后的受试者 ID
"Date_timeline",  # 时间线日期
"VISCODE",        # 访问代码 (如 bl, m06, m12)
"VISCODE2",       # 扩展访问代码
# ---- 人口学 / 遗传 ----
"PTGENDER",       # 性别
"PTEDUCAT",       # 受教育年限
"GENOTYPE",       # APOE 基因型
# ---- 诊断 ----
"DIAGNOSIS",      # 诊断 (CN / MCI / AD)
# ---- 认知 / 临床 ----
"N1M",            # Word List Learning Immediate Recall (平均分)
"MMSCORE",        # Mini-Mental State Exam (MMSE 总分)
# ---- MRI 结构体积 ----
"ST29SV",         # 左海马体积
"ST88SV",         # 右海马体积
"ST109TS"         # 脑室体积 (Ventricles total volume/score)
))
#
data = complete_ADNI$complete_df
data_sorted <- data %>% arrange(ID_merged, Date_timeline)
mci_visits <- data_sorted %>% filter(DIAGNOSIS == 2)
ad_visits  <- data_sorted %>% filter(DIAGNOSIS == 3)
mci_to_ad_ids <- mci_visits %>%
inner_join(ad_visits, by = "ID_merged") %>%
filter(Date_timeline.x < Date_timeline.y) %>%
distinct(ID_merged) %>%
pull(ID_merged)
cat("✅ 找到 MCI → AD 受试者人数:", length(mci_to_ad_ids), "\n")
mci_baseline <- mci_visits %>%
group_by(ID_merged) %>%
slice(1) %>%
ungroup()
mci_labeled <- mci_baseline %>%
mutate(label = ifelse(ID_merged %in% mci_to_ad_ids, 1, 0))
set.seed(42)
positive_df <- mci_labeled %>% filter(label == 1)
negative_df <- mci_labeled %>% filter(label == 0) %>% sample_n(128)
model_df <- bind_rows(positive_df, negative_df)
predictor_vars <- c("PTGENDER", "PTEDUCAT", "GENOTYPE",
"N1M", "MMSCORE", "ST29SV", "ST88SV", "ST109TS")
model_data <- model_df %>%
select(all_of(predictor_vars), label) %>%
na.omit()
model_data$label <- factor(ifelse(model_data$label == 1, "Progressor", "Stable"),
levels = c("Progressor", "Stable"))
# --------------------- #
# 🔄 CV 设置
# --------------------- #
cv_control <- trainControl(
method = "cv", number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = "final"
)
# --------------------- #
# ⚙️ 模型训练
# --------------------- #
set.seed(123)
glm_model <- train(label ~ ., data = model_data,
method = "glm", family = binomial(),
trControl = cv_control, metric = "ROC")
set.seed(123)
rf_model <- train(label ~ ., data = model_data,
method = "rf", tuneLength = 5,
trControl = cv_control, metric = "ROC")
set.seed(123)
gbm_model <- train(label ~ ., data = model_data,
method = "gbm", verbose = FALSE, tuneLength = 5,
trControl = cv_control, metric = "ROC")
set.seed(123)
xgb_model <- train(label ~ ., data = model_data,
method = "xgbTree", tuneLength = 5,
trControl = cv_control, metric = "ROC")
# ----------------------------- #
# 📊 提取性能指标（含 AUC_SD）
# ----------------------------- #
suppress_predict <- function(model, name){
suppressWarnings({
preds <- model$pred
tune_cols <- intersect(names(preds), names(model$bestTune))
# 只保留最佳超参数组合
if (length(tune_cols) > 0){
for (tc in tune_cols){
preds <- preds[preds[[tc]] == model$bestTune[[tc]], ]
}
}
# 每折 confusion matrix
cm_list <- lapply(split(preds, preds$Resample), function(df){
cm <- confusionMatrix(df$pred, df$obs, positive = "Progressor")
data.frame(
Accuracy = cm$overall["Accuracy"],
Sensitivity = cm$byClass["Sensitivity"],
Specificity = cm$byClass["Specificity"]
)
})
cm_df <- bind_rows(cm_list)
# 每折 AUC，强制转 numeric
auc_list <- preds %>%
group_by(Resample) %>%
summarise(
auc = as.numeric(roc(response = obs, predictor = Progressor,
levels = c("Stable", "Progressor"), direction = "<")$auc),
.groups = "drop"
)
auc_mean <- mean(auc_list$auc)
auc_sd   <- sd(auc_list$auc)
# 返回性能指标
data.frame(
Model = name,
AUC = auc_mean,
AUC_SD = auc_sd,
Accuracy = mean(cm_df$Accuracy),
Accuracy_SD = sd(cm_df$Accuracy),
Sensitivity = mean(cm_df$Sensitivity),
Sensitivity_SD = sd(cm_df$Sensitivity),
Specificity = mean(cm_df$Specificity),
Specificity_SD = sd(cm_df$Specificity)
)
})
}
perf_summary <- bind_rows(
suppress_predict(glm_model, "Logistic Regression"),
suppress_predict(rf_model,  "Random Forest"),
suppress_predict(gbm_model, "GBM"),
suppress_predict(xgb_model, "XGBoost")
)
perf_summary_rounded <- perf_summary %>%
mutate(across(where(is.numeric), ~ round(.x, 3)))
# 🧾 输出性能表格
perf_summary_rounded %>%
kable(caption = "5-Fold CV Model Performance Summary", align = 'c') %>%
kable_styling(full_width = FALSE)
# --------------------- #
# 📈 ROC 曲线可视化
# --------------------- #
get_roc_df <- function(model, name){
preds <- model$pred
tune_cols <- intersect(names(preds), names(model$bestTune))
if (length(tune_cols) > 0){
for (tc in tune_cols){
preds <- preds[preds[[tc]] == model$bestTune[[tc]], ]
}
}
roc_obj <- roc(response = preds$obs, predictor = preds$Progressor,
levels = c("Stable", "Progressor"), direction = "<")
data.frame(FPR = 1 - roc_obj$specificities,
TPR = roc_obj$sensitivities,
Model = name)
}
roc_df <- bind_rows(
get_roc_df(glm_model,  "Logistic Regression"),
get_roc_df(rf_model,   "Random Forest"),
get_roc_df(gbm_model,  "GBM"),
get_roc_df(xgb_model,  "XGBoost")
)
ggplot(roc_df, aes(x = FPR, y = TPR, color = Model)) +
geom_line(size = 1.2) +
geom_abline(linetype = "dashed", color = "gray") +
theme_minimal() +
labs(title = "MCI → AD Progression Prediction ROC Curves",
x = "False Positive Rate", y = "True Positive Rate") +
theme(legend.position = "bottom")
ADNI_table
library(dplyr)
library(caret)
library(pROC)
library(ggplot2)
library(knitr)
library(kableExtra)
library(devtools)
library(ADMerge)
# library(dplyr)
library(knitr)
# library(readxl)
library(kableExtra)
ADNI_table = get_src_table(path = "ADNI Demonstration",
ID_usr_list = c("RID"),
DATE_usr_list = c("EXAMDATE","VISDATE"),
non_longitudinal_list = c("APOERES_ADNI_1_2_3"),
IS_overlap_list = c("FALSE"),
WINDOW_list = c(366))
ADNI_table
plot_files(path = "ADNI Demonstration", dict_src = ADNI_table, study_type = "ADNI",date_type = "Date")
ADNI_merged = ad_merge(path = "ADNI_Demonstration",
DATE_type = "Date",
timeline_file = "ADNI_Cognitive",
dict_src = ADNI_table)
#
complete_ADNI = review_complete(ADNI_merged, check_cols = c(
# ---- 基本 ID / 时间 ----
"ID_merged",      # 合并后的受试者 ID
"Date_timeline",  # 时间线日期
"VISCODE",        # 访问代码 (如 bl, m06, m12)
"VISCODE2",       # 扩展访问代码
# ---- 人口学 / 遗传 ----
"PTGENDER",       # 性别
"PTEDUCAT",       # 受教育年限
"GENOTYPE",       # APOE 基因型
# ---- 诊断 ----
"DIAGNOSIS",      # 诊断 (CN / MCI / AD)
# ---- 认知 / 临床 ----
"N1M",            # Word List Learning Immediate Recall (平均分)
"MMSCORE",        # Mini-Mental State Exam (MMSE 总分)
# ---- MRI 结构体积 ----
"ST29SV",         # 左海马体积
"ST88SV",         # 右海马体积
"ST109TS"         # 脑室体积 (Ventricles total volume/score)
))
#
data = complete_ADNI$complete_df
data_sorted <- data %>% arrange(ID_merged, Date_timeline)
mci_visits <- data_sorted %>% filter(DIAGNOSIS == 2)
ad_visits  <- data_sorted %>% filter(DIAGNOSIS == 3)
mci_to_ad_ids <- mci_visits %>%
inner_join(ad_visits, by = "ID_merged") %>%
filter(Date_timeline.x < Date_timeline.y) %>%
distinct(ID_merged) %>%
pull(ID_merged)
cat("✅ 找到 MCI → AD 受试者人数:", length(mci_to_ad_ids), "\n")
mci_baseline <- mci_visits %>%
group_by(ID_merged) %>%
slice(1) %>%
ungroup()
mci_labeled <- mci_baseline %>%
mutate(label = ifelse(ID_merged %in% mci_to_ad_ids, 1, 0))
set.seed(42)
positive_df <- mci_labeled %>% filter(label == 1)
negative_df <- mci_labeled %>% filter(label == 0) %>% sample_n(128)
model_df <- bind_rows(positive_df, negative_df)
predictor_vars <- c("PTGENDER", "PTEDUCAT", "GENOTYPE",
"N1M", "MMSCORE", "ST29SV", "ST88SV", "ST109TS")
model_data <- model_df %>%
select(all_of(predictor_vars), label) %>%
na.omit()
model_data$label <- factor(ifelse(model_data$label == 1, "Progressor", "Stable"),
levels = c("Progressor", "Stable"))
# --------------------- #
# 🔄 CV 设置
# --------------------- #
cv_control <- trainControl(
method = "cv", number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = "final"
)
# --------------------- #
# ⚙️ 模型训练
# --------------------- #
set.seed(123)
glm_model <- train(label ~ ., data = model_data,
method = "glm", family = binomial(),
trControl = cv_control, metric = "ROC")
set.seed(123)
rf_model <- train(label ~ ., data = model_data,
method = "rf", tuneLength = 5,
trControl = cv_control, metric = "ROC")
set.seed(123)
gbm_model <- train(label ~ ., data = model_data,
method = "gbm", verbose = FALSE, tuneLength = 5,
trControl = cv_control, metric = "ROC")
set.seed(123)
xgb_model <- train(label ~ ., data = model_data,
method = "xgbTree", tuneLength = 5,
trControl = cv_control, metric = "ROC")
# ----------------------------- #
# 📊 提取性能指标（含 AUC_SD）
# ----------------------------- #
suppress_predict <- function(model, name){
suppressWarnings({
preds <- model$pred
tune_cols <- intersect(names(preds), names(model$bestTune))
# 只保留最佳超参数组合
if (length(tune_cols) > 0){
for (tc in tune_cols){
preds <- preds[preds[[tc]] == model$bestTune[[tc]], ]
}
}
# 每折 confusion matrix
cm_list <- lapply(split(preds, preds$Resample), function(df){
cm <- confusionMatrix(df$pred, df$obs, positive = "Progressor")
data.frame(
Accuracy = cm$overall["Accuracy"],
Sensitivity = cm$byClass["Sensitivity"],
Specificity = cm$byClass["Specificity"]
)
})
cm_df <- bind_rows(cm_list)
# 每折 AUC，强制转 numeric
auc_list <- preds %>%
group_by(Resample) %>%
summarise(
auc = as.numeric(roc(response = obs, predictor = Progressor,
levels = c("Stable", "Progressor"), direction = "<")$auc),
.groups = "drop"
)
auc_mean <- mean(auc_list$auc)
auc_sd   <- sd(auc_list$auc)
# 返回性能指标
data.frame(
Model = name,
AUC = auc_mean,
AUC_SD = auc_sd,
Accuracy = mean(cm_df$Accuracy),
Accuracy_SD = sd(cm_df$Accuracy),
Sensitivity = mean(cm_df$Sensitivity),
Sensitivity_SD = sd(cm_df$Sensitivity),
Specificity = mean(cm_df$Specificity),
Specificity_SD = sd(cm_df$Specificity)
)
})
}
perf_summary <- bind_rows(
suppress_predict(glm_model, "Logistic Regression"),
suppress_predict(rf_model,  "Random Forest"),
suppress_predict(gbm_model, "GBM"),
suppress_predict(xgb_model, "XGBoost")
)
perf_summary_rounded <- perf_summary %>%
mutate(across(where(is.numeric), ~ round(.x, 3)))
# 🧾 输出性能表格
perf_summary_rounded %>%
kable(caption = "5-Fold CV Model Performance Summary", align = 'c') %>%
kable_styling(full_width = FALSE)
# --------------------- #
# 📈 ROC 曲线可视化
# --------------------- #
get_roc_df <- function(model, name){
preds <- model$pred
tune_cols <- intersect(names(preds), names(model$bestTune))
if (length(tune_cols) > 0){
for (tc in tune_cols){
preds <- preds[preds[[tc]] == model$bestTune[[tc]], ]
}
}
roc_obj <- roc(response = preds$obs, predictor = preds$Progressor,
levels = c("Stable", "Progressor"), direction = "<")
data.frame(FPR = 1 - roc_obj$specificities,
TPR = roc_obj$sensitivities,
Model = name)
}
roc_df <- bind_rows(
get_roc_df(glm_model,  "Logistic Regression"),
get_roc_df(rf_model,   "Random Forest"),
get_roc_df(gbm_model,  "GBM"),
get_roc_df(xgb_model,  "XGBoost")
)
ggplot(roc_df, aes(x = FPR, y = TPR, color = Model)) +
geom_line(size = 1.2) +
geom_abline(linetype = "dashed", color = "gray") +
theme_minimal() +
labs(title = "MCI → AD Progression Prediction ROC Curves",
x = "False Positive Rate", y = "True Positive Rate") +
theme(legend.position = "bottom")
BIOCARD_table
library(dplyr)
library(survival)
library(survminer)
library(gtsummary)
library(broom)
library(ggplot2)
library(ADMerge)
library(dplyr)
BIOCARD_table =get_src_table(path = "/Users/shijia.zhang/Downloads/BIOCARD August 2021 internal",
ID_usr_list = c("SUBJECT_ID"),
DATE_usr_list = c("VISITDATE","DIAGDATE","date","MRIDATE"),
IS_overlap_list = c("FALSE"),
WINDOW_list = c(366))
BIOCARD_table
Biocard_merged = ad_merge(path = "/Users/shijia.zhang/Downloads/BIOCARD August 2021 internal",DATE_type = "Date",timeline_file = "BIOCARD_CognitiveData",dict_src = BIOCARD_table)
Biocard_merged$analysis_data
Biocard_merged$analysis_data %>% distinct(ID_merged)
