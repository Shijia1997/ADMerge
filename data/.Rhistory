classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = "final"
)
# --------------------- #
# âš™ï¸ æ¨¡å‹è®­ç»ƒ
# --------------------- #
set.seed(123)
glm_model <- train(label ~ ., data = model_data,
method = "glm", family = binomial(),
trControl = cv_control, metric = "ROC")
set.seed(123)
rf_model <- train(label ~ ., data = model_data,
method = "rf", tuneLength = 5,
trControl = cv_control, metric = "ROC")
set.seed(123)
gbm_model <- train(label ~ ., data = model_data,
method = "gbm", verbose = FALSE, tuneLength = 5,
trControl = cv_control, metric = "ROC")
set.seed(123)
xgb_model <- train(label ~ ., data = model_data,
method = "xgbTree", tuneLength = 5,
trControl = cv_control, metric = "ROC")
# ----------------------------- #
# ğŸ“Š æå–æ€§èƒ½æŒ‡æ ‡ï¼ˆå« AUC_SDï¼‰
# ----------------------------- #
suppress_predict <- function(model, name){
suppressWarnings({
preds <- model$pred
tune_cols <- intersect(names(preds), names(model$bestTune))
# åªä¿ç•™æœ€ä½³è¶…å‚æ•°ç»„åˆ
if (length(tune_cols) > 0){
for (tc in tune_cols){
preds <- preds[preds[[tc]] == model$bestTune[[tc]], ]
}
}
# æ¯æŠ˜ confusion matrix
cm_list <- lapply(split(preds, preds$Resample), function(df){
cm <- confusionMatrix(df$pred, df$obs, positive = "Progressor")
data.frame(
Accuracy = cm$overall["Accuracy"],
Sensitivity = cm$byClass["Sensitivity"],
Specificity = cm$byClass["Specificity"]
)
})
cm_df <- bind_rows(cm_list)
# æ¯æŠ˜ AUCï¼Œå¼ºåˆ¶è½¬ numeric
auc_list <- preds %>%
group_by(Resample) %>%
summarise(
auc = as.numeric(roc(response = obs, predictor = Progressor,
levels = c("Stable", "Progressor"), direction = "<")$auc),
.groups = "drop"
)
auc_mean <- mean(auc_list$auc)
auc_sd   <- sd(auc_list$auc)
# è¿”å›æ€§èƒ½æŒ‡æ ‡
data.frame(
Model = name,
AUC = auc_mean,
AUC_SD = auc_sd,
Accuracy = mean(cm_df$Accuracy),
Accuracy_SD = sd(cm_df$Accuracy),
Sensitivity = mean(cm_df$Sensitivity),
Sensitivity_SD = sd(cm_df$Sensitivity),
Specificity = mean(cm_df$Specificity),
Specificity_SD = sd(cm_df$Specificity)
)
})
}
perf_summary <- bind_rows(
suppress_predict(glm_model, "Logistic Regression"),
suppress_predict(rf_model,  "Random Forest"),
suppress_predict(gbm_model, "GBM"),
suppress_predict(xgb_model, "XGBoost")
)
perf_summary_rounded <- perf_summary %>%
mutate(across(where(is.numeric), ~ round(.x, 3)))
# ğŸ§¾ è¾“å‡ºæ€§èƒ½è¡¨æ ¼
perf_summary_rounded %>%
kable(caption = "5-Fold CV Model Performance Summary", align = 'c') %>%
kable_styling(full_width = FALSE)
# --------------------- #
# ğŸ“ˆ ROC æ›²çº¿å¯è§†åŒ–
# --------------------- #
get_roc_df <- function(model, name){
preds <- model$pred
tune_cols <- intersect(names(preds), names(model$bestTune))
if (length(tune_cols) > 0){
for (tc in tune_cols){
preds <- preds[preds[[tc]] == model$bestTune[[tc]], ]
}
}
roc_obj <- roc(response = preds$obs, predictor = preds$Progressor,
levels = c("Stable", "Progressor"), direction = "<")
data.frame(FPR = 1 - roc_obj$specificities,
TPR = roc_obj$sensitivities,
Model = name)
}
roc_df <- bind_rows(
get_roc_df(glm_model,  "Logistic Regression"),
get_roc_df(rf_model,   "Random Forest"),
get_roc_df(gbm_model,  "GBM"),
get_roc_df(xgb_model,  "XGBoost")
)
ggplot(roc_df, aes(x = FPR, y = TPR, color = Model)) +
geom_line(size = 1.2) +
geom_abline(linetype = "dashed", color = "gray") +
theme_minimal() +
labs(title = "MCI â†’ AD Progression Prediction ROC Curves",
x = "False Positive Rate", y = "True Positive Rate") +
theme(legend.position = "bottom")
ADNI_table
library(dplyr)
library(caret)
library(pROC)
library(ggplot2)
library(knitr)
library(kableExtra)
library(devtools)
library(ADMerge)
# library(dplyr)
library(knitr)
# library(readxl)
library(kableExtra)
ADNI_table = get_src_table(path = "ADNI Demonstration",
ID_usr_list = c("RID"),
DATE_usr_list = c("EXAMDATE","VISDATE"),
non_longitudinal_list = c("APOERES_ADNI_1_2_3"),
IS_overlap_list = c("FALSE"),
WINDOW_list = c(366))
ADNI_table
plot_files(path = "ADNI Demonstration", dict_src = ADNI_table, study_type = "ADNI",date_type = "Date")
ADNI_merged = ad_merge(path = "ADNI_Demonstration",
DATE_type = "Date",
timeline_file = "ADNI_Cognitive",
dict_src = ADNI_table)
#
complete_ADNI = review_complete(ADNI_merged, check_cols = c(
# ---- åŸºæœ¬ ID / æ—¶é—´ ----
"ID_merged",      # åˆå¹¶åçš„å—è¯•è€… ID
"Date_timeline",  # æ—¶é—´çº¿æ—¥æœŸ
"VISCODE",        # è®¿é—®ä»£ç  (å¦‚ bl, m06, m12)
"VISCODE2",       # æ‰©å±•è®¿é—®ä»£ç 
# ---- äººå£å­¦ / é—ä¼  ----
"PTGENDER",       # æ€§åˆ«
"PTEDUCAT",       # å—æ•™è‚²å¹´é™
"GENOTYPE",       # APOE åŸºå› å‹
# ---- è¯Šæ–­ ----
"DIAGNOSIS",      # è¯Šæ–­ (CN / MCI / AD)
# ---- è®¤çŸ¥ / ä¸´åºŠ ----
"N1M",            # Word List Learning Immediate Recall (å¹³å‡åˆ†)
"MMSCORE",        # Mini-Mental State Exam (MMSE æ€»åˆ†)
# ---- MRI ç»“æ„ä½“ç§¯ ----
"ST29SV",         # å·¦æµ·é©¬ä½“ç§¯
"ST88SV",         # å³æµ·é©¬ä½“ç§¯
"ST109TS"         # è„‘å®¤ä½“ç§¯ (Ventricles total volume/score)
))
#
data = complete_ADNI$complete_df
data_sorted <- data %>% arrange(ID_merged, Date_timeline)
mci_visits <- data_sorted %>% filter(DIAGNOSIS == 2)
ad_visits  <- data_sorted %>% filter(DIAGNOSIS == 3)
mci_to_ad_ids <- mci_visits %>%
inner_join(ad_visits, by = "ID_merged") %>%
filter(Date_timeline.x < Date_timeline.y) %>%
distinct(ID_merged) %>%
pull(ID_merged)
cat("âœ… æ‰¾åˆ° MCI â†’ AD å—è¯•è€…äººæ•°:", length(mci_to_ad_ids), "\n")
mci_baseline <- mci_visits %>%
group_by(ID_merged) %>%
slice(1) %>%
ungroup()
mci_labeled <- mci_baseline %>%
mutate(label = ifelse(ID_merged %in% mci_to_ad_ids, 1, 0))
set.seed(42)
positive_df <- mci_labeled %>% filter(label == 1)
negative_df <- mci_labeled %>% filter(label == 0) %>% sample_n(128)
model_df <- bind_rows(positive_df, negative_df)
predictor_vars <- c("PTGENDER", "PTEDUCAT", "GENOTYPE",
"N1M", "MMSCORE", "ST29SV", "ST88SV", "ST109TS")
model_data <- model_df %>%
select(all_of(predictor_vars), label) %>%
na.omit()
model_data$label <- factor(ifelse(model_data$label == 1, "Progressor", "Stable"),
levels = c("Progressor", "Stable"))
# --------------------- #
# ğŸ”„ CV è®¾ç½®
# --------------------- #
cv_control <- trainControl(
method = "cv", number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = "final"
)
# --------------------- #
# âš™ï¸ æ¨¡å‹è®­ç»ƒ
# --------------------- #
set.seed(123)
glm_model <- train(label ~ ., data = model_data,
method = "glm", family = binomial(),
trControl = cv_control, metric = "ROC")
set.seed(123)
rf_model <- train(label ~ ., data = model_data,
method = "rf", tuneLength = 5,
trControl = cv_control, metric = "ROC")
set.seed(123)
gbm_model <- train(label ~ ., data = model_data,
method = "gbm", verbose = FALSE, tuneLength = 5,
trControl = cv_control, metric = "ROC")
set.seed(123)
xgb_model <- train(label ~ ., data = model_data,
method = "xgbTree", tuneLength = 5,
trControl = cv_control, metric = "ROC")
# ----------------------------- #
# ğŸ“Š æå–æ€§èƒ½æŒ‡æ ‡ï¼ˆå« AUC_SDï¼‰
# ----------------------------- #
suppress_predict <- function(model, name){
suppressWarnings({
preds <- model$pred
tune_cols <- intersect(names(preds), names(model$bestTune))
# åªä¿ç•™æœ€ä½³è¶…å‚æ•°ç»„åˆ
if (length(tune_cols) > 0){
for (tc in tune_cols){
preds <- preds[preds[[tc]] == model$bestTune[[tc]], ]
}
}
# æ¯æŠ˜ confusion matrix
cm_list <- lapply(split(preds, preds$Resample), function(df){
cm <- confusionMatrix(df$pred, df$obs, positive = "Progressor")
data.frame(
Accuracy = cm$overall["Accuracy"],
Sensitivity = cm$byClass["Sensitivity"],
Specificity = cm$byClass["Specificity"]
)
})
cm_df <- bind_rows(cm_list)
# æ¯æŠ˜ AUCï¼Œå¼ºåˆ¶è½¬ numeric
auc_list <- preds %>%
group_by(Resample) %>%
summarise(
auc = as.numeric(roc(response = obs, predictor = Progressor,
levels = c("Stable", "Progressor"), direction = "<")$auc),
.groups = "drop"
)
auc_mean <- mean(auc_list$auc)
auc_sd   <- sd(auc_list$auc)
# è¿”å›æ€§èƒ½æŒ‡æ ‡
data.frame(
Model = name,
AUC = auc_mean,
AUC_SD = auc_sd,
Accuracy = mean(cm_df$Accuracy),
Accuracy_SD = sd(cm_df$Accuracy),
Sensitivity = mean(cm_df$Sensitivity),
Sensitivity_SD = sd(cm_df$Sensitivity),
Specificity = mean(cm_df$Specificity),
Specificity_SD = sd(cm_df$Specificity)
)
})
}
perf_summary <- bind_rows(
suppress_predict(glm_model, "Logistic Regression"),
suppress_predict(rf_model,  "Random Forest"),
suppress_predict(gbm_model, "GBM"),
suppress_predict(xgb_model, "XGBoost")
)
perf_summary_rounded <- perf_summary %>%
mutate(across(where(is.numeric), ~ round(.x, 3)))
# ğŸ§¾ è¾“å‡ºæ€§èƒ½è¡¨æ ¼
perf_summary_rounded %>%
kable(caption = "5-Fold CV Model Performance Summary", align = 'c') %>%
kable_styling(full_width = FALSE)
# --------------------- #
# ğŸ“ˆ ROC æ›²çº¿å¯è§†åŒ–
# --------------------- #
get_roc_df <- function(model, name){
preds <- model$pred
tune_cols <- intersect(names(preds), names(model$bestTune))
if (length(tune_cols) > 0){
for (tc in tune_cols){
preds <- preds[preds[[tc]] == model$bestTune[[tc]], ]
}
}
roc_obj <- roc(response = preds$obs, predictor = preds$Progressor,
levels = c("Stable", "Progressor"), direction = "<")
data.frame(FPR = 1 - roc_obj$specificities,
TPR = roc_obj$sensitivities,
Model = name)
}
roc_df <- bind_rows(
get_roc_df(glm_model,  "Logistic Regression"),
get_roc_df(rf_model,   "Random Forest"),
get_roc_df(gbm_model,  "GBM"),
get_roc_df(xgb_model,  "XGBoost")
)
ggplot(roc_df, aes(x = FPR, y = TPR, color = Model)) +
geom_line(size = 1.2) +
geom_abline(linetype = "dashed", color = "gray") +
theme_minimal() +
labs(title = "MCI â†’ AD Progression Prediction ROC Curves",
x = "False Positive Rate", y = "True Positive Rate") +
theme(legend.position = "bottom")
ADNI_table
library(dplyr)
library(caret)
library(pROC)
library(ggplot2)
library(knitr)
library(kableExtra)
library(devtools)
library(ADMerge)
# library(dplyr)
library(knitr)
# library(readxl)
library(kableExtra)
ADNI_table = get_src_table(path = "ADNI Demonstration",
ID_usr_list = c("RID"),
DATE_usr_list = c("EXAMDATE","VISDATE"),
non_longitudinal_list = c("APOERES_ADNI_1_2_3"),
IS_overlap_list = c("FALSE"),
WINDOW_list = c(366))
ADNI_table
plot_files(path = "ADNI Demonstration", dict_src = ADNI_table, study_type = "ADNI",date_type = "Date")
ADNI_merged = ad_merge(path = "ADNI_Demonstration",
DATE_type = "Date",
timeline_file = "ADNI_Cognitive",
dict_src = ADNI_table)
#
complete_ADNI = review_complete(ADNI_merged, check_cols = c(
# ---- åŸºæœ¬ ID / æ—¶é—´ ----
"ID_merged",      # åˆå¹¶åçš„å—è¯•è€… ID
"Date_timeline",  # æ—¶é—´çº¿æ—¥æœŸ
"VISCODE",        # è®¿é—®ä»£ç  (å¦‚ bl, m06, m12)
"VISCODE2",       # æ‰©å±•è®¿é—®ä»£ç 
# ---- äººå£å­¦ / é—ä¼  ----
"PTGENDER",       # æ€§åˆ«
"PTEDUCAT",       # å—æ•™è‚²å¹´é™
"GENOTYPE",       # APOE åŸºå› å‹
# ---- è¯Šæ–­ ----
"DIAGNOSIS",      # è¯Šæ–­ (CN / MCI / AD)
# ---- è®¤çŸ¥ / ä¸´åºŠ ----
"N1M",            # Word List Learning Immediate Recall (å¹³å‡åˆ†)
"MMSCORE",        # Mini-Mental State Exam (MMSE æ€»åˆ†)
# ---- MRI ç»“æ„ä½“ç§¯ ----
"ST29SV",         # å·¦æµ·é©¬ä½“ç§¯
"ST88SV",         # å³æµ·é©¬ä½“ç§¯
"ST109TS"         # è„‘å®¤ä½“ç§¯ (Ventricles total volume/score)
))
#
data = complete_ADNI$complete_df
data_sorted <- data %>% arrange(ID_merged, Date_timeline)
mci_visits <- data_sorted %>% filter(DIAGNOSIS == 2)
ad_visits  <- data_sorted %>% filter(DIAGNOSIS == 3)
mci_to_ad_ids <- mci_visits %>%
inner_join(ad_visits, by = "ID_merged") %>%
filter(Date_timeline.x < Date_timeline.y) %>%
distinct(ID_merged) %>%
pull(ID_merged)
cat("âœ… æ‰¾åˆ° MCI â†’ AD å—è¯•è€…äººæ•°:", length(mci_to_ad_ids), "\n")
mci_baseline <- mci_visits %>%
group_by(ID_merged) %>%
slice(1) %>%
ungroup()
mci_labeled <- mci_baseline %>%
mutate(label = ifelse(ID_merged %in% mci_to_ad_ids, 1, 0))
set.seed(42)
positive_df <- mci_labeled %>% filter(label == 1)
negative_df <- mci_labeled %>% filter(label == 0) %>% sample_n(128)
model_df <- bind_rows(positive_df, negative_df)
predictor_vars <- c("PTGENDER", "PTEDUCAT", "GENOTYPE",
"N1M", "MMSCORE", "ST29SV", "ST88SV", "ST109TS")
model_data <- model_df %>%
select(all_of(predictor_vars), label) %>%
na.omit()
model_data$label <- factor(ifelse(model_data$label == 1, "Progressor", "Stable"),
levels = c("Progressor", "Stable"))
# --------------------- #
# ğŸ”„ CV è®¾ç½®
# --------------------- #
cv_control <- trainControl(
method = "cv", number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = "final"
)
# --------------------- #
# âš™ï¸ æ¨¡å‹è®­ç»ƒ
# --------------------- #
set.seed(123)
glm_model <- train(label ~ ., data = model_data,
method = "glm", family = binomial(),
trControl = cv_control, metric = "ROC")
set.seed(123)
rf_model <- train(label ~ ., data = model_data,
method = "rf", tuneLength = 5,
trControl = cv_control, metric = "ROC")
set.seed(123)
gbm_model <- train(label ~ ., data = model_data,
method = "gbm", verbose = FALSE, tuneLength = 5,
trControl = cv_control, metric = "ROC")
set.seed(123)
xgb_model <- train(label ~ ., data = model_data,
method = "xgbTree", tuneLength = 5,
trControl = cv_control, metric = "ROC")
# ----------------------------- #
# ğŸ“Š æå–æ€§èƒ½æŒ‡æ ‡ï¼ˆå« AUC_SDï¼‰
# ----------------------------- #
suppress_predict <- function(model, name){
suppressWarnings({
preds <- model$pred
tune_cols <- intersect(names(preds), names(model$bestTune))
# åªä¿ç•™æœ€ä½³è¶…å‚æ•°ç»„åˆ
if (length(tune_cols) > 0){
for (tc in tune_cols){
preds <- preds[preds[[tc]] == model$bestTune[[tc]], ]
}
}
# æ¯æŠ˜ confusion matrix
cm_list <- lapply(split(preds, preds$Resample), function(df){
cm <- confusionMatrix(df$pred, df$obs, positive = "Progressor")
data.frame(
Accuracy = cm$overall["Accuracy"],
Sensitivity = cm$byClass["Sensitivity"],
Specificity = cm$byClass["Specificity"]
)
})
cm_df <- bind_rows(cm_list)
# æ¯æŠ˜ AUCï¼Œå¼ºåˆ¶è½¬ numeric
auc_list <- preds %>%
group_by(Resample) %>%
summarise(
auc = as.numeric(roc(response = obs, predictor = Progressor,
levels = c("Stable", "Progressor"), direction = "<")$auc),
.groups = "drop"
)
auc_mean <- mean(auc_list$auc)
auc_sd   <- sd(auc_list$auc)
# è¿”å›æ€§èƒ½æŒ‡æ ‡
data.frame(
Model = name,
AUC = auc_mean,
AUC_SD = auc_sd,
Accuracy = mean(cm_df$Accuracy),
Accuracy_SD = sd(cm_df$Accuracy),
Sensitivity = mean(cm_df$Sensitivity),
Sensitivity_SD = sd(cm_df$Sensitivity),
Specificity = mean(cm_df$Specificity),
Specificity_SD = sd(cm_df$Specificity)
)
})
}
perf_summary <- bind_rows(
suppress_predict(glm_model, "Logistic Regression"),
suppress_predict(rf_model,  "Random Forest"),
suppress_predict(gbm_model, "GBM"),
suppress_predict(xgb_model, "XGBoost")
)
perf_summary_rounded <- perf_summary %>%
mutate(across(where(is.numeric), ~ round(.x, 3)))
# ğŸ§¾ è¾“å‡ºæ€§èƒ½è¡¨æ ¼
perf_summary_rounded %>%
kable(caption = "5-Fold CV Model Performance Summary", align = 'c') %>%
kable_styling(full_width = FALSE)
# --------------------- #
# ğŸ“ˆ ROC æ›²çº¿å¯è§†åŒ–
# --------------------- #
get_roc_df <- function(model, name){
preds <- model$pred
tune_cols <- intersect(names(preds), names(model$bestTune))
if (length(tune_cols) > 0){
for (tc in tune_cols){
preds <- preds[preds[[tc]] == model$bestTune[[tc]], ]
}
}
roc_obj <- roc(response = preds$obs, predictor = preds$Progressor,
levels = c("Stable", "Progressor"), direction = "<")
data.frame(FPR = 1 - roc_obj$specificities,
TPR = roc_obj$sensitivities,
Model = name)
}
roc_df <- bind_rows(
get_roc_df(glm_model,  "Logistic Regression"),
get_roc_df(rf_model,   "Random Forest"),
get_roc_df(gbm_model,  "GBM"),
get_roc_df(xgb_model,  "XGBoost")
)
ggplot(roc_df, aes(x = FPR, y = TPR, color = Model)) +
geom_line(size = 1.2) +
geom_abline(linetype = "dashed", color = "gray") +
theme_minimal() +
labs(title = "MCI â†’ AD Progression Prediction ROC Curves",
x = "False Positive Rate", y = "True Positive Rate") +
theme(legend.position = "bottom")
BIOCARD_table
library(dplyr)
library(survival)
library(survminer)
library(gtsummary)
library(broom)
library(ggplot2)
library(ADMerge)
library(dplyr)
BIOCARD_table =get_src_table(path = "/Users/shijia.zhang/Downloads/BIOCARD August 2021 internal",
ID_usr_list = c("SUBJECT_ID"),
DATE_usr_list = c("VISITDATE","DIAGDATE","date","MRIDATE"),
IS_overlap_list = c("FALSE"),
WINDOW_list = c(366))
BIOCARD_table
Biocard_merged = ad_merge(path = "/Users/shijia.zhang/Downloads/BIOCARD August 2021 internal",DATE_type = "Date",timeline_file = "BIOCARD_CognitiveData",dict_src = BIOCARD_table)
Biocard_merged$analysis_data
Biocard_merged$analysis_data %>% distinct(ID_merged)
